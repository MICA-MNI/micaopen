{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178544bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import nibabel as nib\n",
    "import numpy.ma as ma\n",
    "from numpy import savetxt\n",
    "import os\n",
    "from brainspace.datasets import load_group_fc, load_parcellation, load_conte69\n",
    "import brainstat.mesh.data as mesh\n",
    "\n",
    "#input variables and path defs\n",
    "#workDir = '/data_/mica1/03_projects/hans/7T/func/'\n",
    "workDir='/host/percy/local_raid/hans/amyg/func/'\n",
    "outDir = str(workDir+'outputs/')\n",
    "volDir = '/data_/mica3/BIDS_PNC/rawdata/'\n",
    "regDir = '/data_/mica1/03_projects/hans/7T/registration/'\n",
    "outDir1 = '/host/percy/local_raid/hans/amyg/func/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de02671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "138\n",
      "138\n",
      "138\n",
      "inverted\n",
      "(275, 138)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
      "this is the shape of the timeseries\n",
      "(275, 64984)\n",
      " 275 x 1 surfaces to smooth, % remaining: 100 \n",
      "90 80 70 60 50 40 30 20 10 0 Done\n",
      " 275 x 1 surfaces to smooth, % remaining: 100 \n",
      "90 80 70 60 50 40 30 20 10 0 Done\n",
      "64984\n",
      "(4, 64984)\n",
      "106\n",
      "106\n",
      "106\n",
      "106\n",
      "not inverted\n",
      "(275, 106)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "this is the shape of the timeseries\n",
      "(275, 64984)\n",
      " 275 x 1 surfaces to smooth, % remaining: 100 \n",
      "90 80 70 60 50 40 30 20 10 0 Done\n",
      " 275 x 1 surfaces to smooth, % remaining: 100 \n",
      "90 80 70 60 50 40 30 20 10 0 Done\n",
      "64984\n",
      "(4, 64984)\n"
     ]
    }
   ],
   "source": [
    "##retrieve U1 and U2 maps in func space to partition them into groups for the highest and lowest 25% values\n",
    "#iterate through each subject\n",
    "\n",
    "###threshold the top and bttom X% values of U1 and U2\n",
    "thrshhld=25\n",
    "gauss=10\n",
    "\n",
    "#subjList=[1,4,5,6,7,9,10,11,12,13]\n",
    "subjList=[1,4]\n",
    "\n",
    "for i in subjList:\n",
    "    if i<10:\n",
    "        subject = str('PNC00'+str(i))\n",
    "    else:\n",
    "        subject = str('PNC0'+str(i))\n",
    "    #swith U1 and U2 of PNC011 because of UMAP orientation\n",
    "    if i == 11:\n",
    "        mapNif1 = nib.load(workDir+subject+'_01_amyg_U2_funcSpace_1sd.nii.gz')\n",
    "        mapU1 = np.array(mapNif1.dataobj)\n",
    "        mapNif2 = nib.load(workDir+subject+'_01_amyg_U1_funcSpace_1sd.nii.gz')\n",
    "        mapU2 = np.array(mapNif2.dataobj)\n",
    "    else:\n",
    "        mapNif1 = nib.load(workDir+subject+'_01_amyg_U1_funcSpace_1sd.nii.gz')\n",
    "        mapU1 = np.array(mapNif1.dataobj)\n",
    "        mapNif2 = nib.load(workDir+subject+'_01_amyg_U2_funcSpace_1sd.nii.gz')\n",
    "        mapU2 = np.array(mapNif2.dataobj)\n",
    "    #retreieve amygdala mask to be able to have array of u1 and u2 values only within amygdala borders\n",
    "    maskNif = nib.load(workDir+subject+'_01_amyg_mask_funcSpace_1sd.nii.gz')\n",
    "    mask = np.array(maskNif.dataobj)\n",
    "    \n",
    "    mask = np.where(mask == 1, 0, 1)\n",
    "    maskBin = mask.tolist()\n",
    "    #print(maskBin[1][1][:])\n",
    "    U1vec = ma.masked_array(mapU1, maskBin)\n",
    "    U1vec = U1vec.compressed()\n",
    "    \n",
    "    U2vec = ma.masked_array(mapU2, maskBin)\n",
    "    U2vec = U2vec.compressed()\n",
    "    \n",
    "    #make array which contains coordinate values along with u1 and u2 values or each voxel\n",
    "    #print(U1vec)\n",
    "    print(len(U1vec))\n",
    "    #print(U2vec)\n",
    "    print(len(U2vec))    \n",
    "    coords=np.zeros((12214,5))\n",
    "    num=0\n",
    "    for i in range(len(mapU1[:,1,1])):\n",
    "        for j in range(len(mapU1[1,:,1])):\n",
    "            for k in range(len(mapU1[1,1,:])):\n",
    "                if mapU1[i,j,k] != 0:\n",
    "                    coords[num,0]=i\n",
    "                    coords[num,1]=j\n",
    "                    coords[num,2]=k\n",
    "                    coords[num,3]=mapU1[i,j,k]\n",
    "                    coords[num,4]=mapU2[i,j,k]\n",
    "                    num=num+1\n",
    "    print(num)\n",
    "    num=0\n",
    "    for i in range(len(mapU2[:,1,1])):\n",
    "        for j in range(len(mapU2[1,:,1])):\n",
    "            for k in range(len(mapU2[1,1,:])):\n",
    "                if mapU1[i,j,k] != 0:\n",
    "                    coords[num,0]=i\n",
    "                    coords[num,1]=j\n",
    "                    coords[num,2]=k\n",
    "                    coords[num,3]=mapU1[i,j,k]\n",
    "                    coords[num,4]=mapU2[i,j,k]\n",
    "                    num=num+1\n",
    "    print(num)\n",
    "    \n",
    "    ###threshold the top and bttom 25% values of U1 and U2\n",
    "    #U1\n",
    "    percentage=0.01*thrshhld\n",
    "    threshold=int(percentage*len(U1vec))\n",
    "    #print(threshold)\n",
    "    \n",
    "    #depending on whether subject have positive or negative correlations with U1 and the two main axes deifned by U1(medial lateral and inferior superior) we will inverse the top of bottom values to stay consistent\n",
    "    #subjects that have negative correlations:PNC001,PNC007,PNC009,PNC012  #make code more generalizable by choosing which ones to inverse by looking directly at the correlation values\n",
    "    if subject == 'PNC001' or subject == 'PNC007' or subject == 'PNC009' or subject == 'PNC012':\n",
    "        u1bot=np.argpartition(U1vec,-threshold)[-threshold:]\n",
    "        u1top=np.argpartition(U1vec,threshold)[:threshold]\n",
    "        print('inverted')\n",
    "    else:\n",
    "        u1top=np.argpartition(U1vec,-threshold)[-threshold:]\n",
    "        u1bot=np.argpartition(U1vec,threshold)[:threshold]\n",
    "        print('not inverted')\n",
    "    u2top=np.argpartition(U2vec,-threshold)[-threshold:]\n",
    "    u2bot=np.argpartition(U2vec,threshold)[:threshold]\n",
    "    path=str(workDir+subject+'_01_amyg_func_space-func_desc-me_timeseries_clean.txt')\n",
    "    tseries=np.loadtxt(path,delimiter=' ')\n",
    "    print(tseries.shape)\n",
    "    #get the average values of all timeseries for each partition (u1top25 U1bot25...)\n",
    "    weights=np.zeros(len(U1vec))\n",
    "    for i in u1top:\n",
    "        weights[i]=1\n",
    "    print(weights)\n",
    "    u1topavg=np.average(tseries,axis=1,weights=weights)\n",
    "\n",
    "    weights=np.zeros(len(U1vec))\n",
    "    for i in u1bot:\n",
    "        weights[i]=1\n",
    "    #print(weights)\n",
    "    u1botavg=np.average(tseries,axis=1,weights=weights)\n",
    "    #print(topavg.shape)\n",
    "\n",
    "    #U2\n",
    "    weights=np.zeros(len(U1vec))\n",
    "    for i in u2top:\n",
    "        weights[i]=1\n",
    "    #print(weights)\n",
    "    u2topavg=np.average(tseries,axis=1,weights=weights)\n",
    "\n",
    "    weights=np.zeros(len(U1vec))\n",
    "    for i in u2bot:\n",
    "        weights[i]=1\n",
    "    #print(weights)\n",
    "    u2botavg=np.average(tseries,axis=1,weights=weights)\n",
    "    #print(topavg.shape)\n",
    "\n",
    "    #retrieve average value for whole amygdala\n",
    "    for i in mask:\n",
    "        weights[i]=1\n",
    "    amyg_timeseries=np.average(tseries,axis=1,weights=weights)\n",
    "    \n",
    "    ###retrieve the cortex timeseries for the subject\n",
    "    funcDir=str('/data_/mica3/BIDS_PNI/derivatives/micapipe_v0.2.0/sub-'+subject+'/ses-01/func/desc-me_task-rest_bold/')\n",
    "    path=str(funcDir+'surf/sub-'+subject+'_ses-01_surf-fsLR-32k_desc-timeseries_clean.shape.gii')\n",
    "    tmp=nib.load(path)\n",
    "    cortexTseries=np.array([x.data for x in tmp.darrays])\n",
    "    cortexTseries=cortexTseries[0]\n",
    "    #cortexTseries=np.loadtxt(path)\n",
    "    \n",
    "    print('this is the shape of the timeseries')\n",
    "    print(cortexTseries.shape)\n",
    "    ###gaussian smooth for the timeseries values\n",
    "    surf_lh, surf_rh = load_conte69()\n",
    "    lh_smooth=mesh.mesh_smooth(cortexTseries[:,:32492],surf_lh,gauss)\n",
    "    rh_smooth=mesh.mesh_smooth(cortexTseries[:,32492:],surf_rh,gauss)\n",
    "    cortexTseries = np.append(lh_smooth,rh_smooth,axis=1)\n",
    "    \n",
    "    \n",
    "    ###get pearson correlation of the avg of top and bottom 25% of u1 nd u2 values with the cortex timeseries\n",
    "    from scipy import stats\n",
    "    Pcorr=np.zeros((4,len(cortexTseries[1,:])))\n",
    "    tmp=np.zeros(len(cortexTseries[1,:]))\n",
    "    #also get the pearson correlation of the avg amygdala values with the cortex timeseries\n",
    "    PcorrAmyg=np.zeros((1,len(cortexTseries[1,:])))\n",
    "    for i in range(len(cortexTseries[1,:])):\n",
    "        res1=stats.pearsonr(u1topavg,cortexTseries[:,i])\n",
    "        res2=stats.pearsonr(u1botavg,cortexTseries[:,i])\n",
    "        res3=stats.pearsonr(u2topavg,cortexTseries[:,i])\n",
    "        res4=stats.pearsonr(u2botavg,cortexTseries[:,i])\n",
    "        Pcorr[0,i]=res1[0]\n",
    "        Pcorr[1,i]=res2[0]\n",
    "        Pcorr[2,i]=res3[0]\n",
    "        Pcorr[3,i]=res4[0]\n",
    "        \n",
    "        #get the pearson correlation of the avg amygdala values with the cortex timeseries\n",
    "        res5=stats.pearsonr(amyg_timeseries,cortexTseries[:,i])\n",
    "        PcorrAmyg[0,i]=res5[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###save all the correlation values in a 4 X 64984 matrix where it is u1top25, u1bot25, u2top25, u2bot25\n",
    "    print(len(cortexTseries[1,:]))\n",
    "    print(Pcorr.shape)\n",
    "    path=str(outDir1+subject+'_U1U2_corr_top_bot_'+str(thrshhld)+'_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    #np.savetxt(path,Pcorr)\n",
    "    \n",
    "    #also save correlation values of whole amyg\n",
    "    path=str(outDir1+subject+'_whole_amyg_corr_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    #np.savetxt(path,PcorrAmyg)\n",
    "\n",
    "    #save correlations with gaussian smooth for the timeseries values\n",
    "    #path=str(outDir1+subject+'_U1U2_corr_top_bot_10_with_cortex_smoothed.txt')\n",
    "    #np.savetxt(path,Pcorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d591253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score for each subject U1 and U2 partitions\n",
    "import scipy.stats as stats\n",
    "def fisher_z(r):\n",
    "    tmp=np.log(1+r)-np.log(1-r)\n",
    "    return 0.5*tmp\n",
    "\n",
    "\n",
    "for i in subjList:\n",
    "    if i<10:\n",
    "        subject = str('PNC00'+str(i))\n",
    "    else:\n",
    "        subject = str('PNC0'+str(i))\n",
    "    pathIn=str(outDir1+subject+'_U1U2_corr_top_bot_'+str(thrshhld)+'_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    Pcorr=np.loadtxt(pathIn)\n",
    "    #Pcorr=np.arctanh(Pcorr)\n",
    "    #Pcorr=np.nan_to_num(Pcorr)\n",
    "    z_connectivity = map(fisher_z,Pcorr)\n",
    "    Pcorr = np.array(list(z_connectivity))\n",
    "\n",
    "    pathOut=str(outDir1+subject+'_U1U2_fisherZ_top_bot_'+str(thrshhld)+'_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    #np.savetxt(pathOut,Pcorr)\n",
    "    \n",
    "    #(Z-score) do the same but with the whole amygdala, z-score the correlations of whole amyg and cortex timeseries\n",
    "    path1=str(outDir1+subject+'_whole_amyg_corr_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    PcorrAmyg=np.loadtxt(path1)\n",
    "    #PcorrAmyg=np.arctanh(PcorrAmyg)\n",
    "    z_connectivity=map(fisher_z,PcorrAmyg)\n",
    "    PcorrAmyg = np.array(list(z_connectivity))\n",
    "\n",
    "    path2=str(outDir1+subject+'_fisherZ_whole_amyg_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    #np.savetxt(path2,PcorrAmyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907eea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export02/data/jessica/miniconda3/envs/python37/lib/python3.7/site-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#makes hemisphere plots of z-scored average pearson correlation of all subject timeseries FOR WHOLE AMYGDALA\n",
    "from brainspace.plotting import plot_hemispheres\n",
    "from brainspace.utils.parcellation import map_to_labels\n",
    "from brainspace.datasets import load_group_fc, load_parcellation, load_conte69\n",
    "import brainstat.mesh.data as mesh\n",
    "from brainstat.datasets import fetch_mask, fetch_template_surface\n",
    "from brainstat.tutorial.utils import fetch_mics_data\n",
    "corr_all_subs=np.zeros((1,10,64984))\n",
    "for count, i in enumerate([1,4,5,6,7,9,10,11,12,13]):\n",
    "    if i<10:\n",
    "        subject = str('PNC00'+str(i))\n",
    "    else:\n",
    "        subject = str('PNC0'+str(i))\n",
    "    path=str(outDir1+subject+'_fisherZ_whole_amyg_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    PcorrAmyg=np.loadtxt(path)\n",
    "    corr_all_subs[0][count][:]=PcorrAmyg\n",
    "\n",
    "PcorrAmyg=np.average(corr_all_subs[0][:][:],axis=0)\n",
    "\n",
    "surf_lh, surf_rh = load_conte69()\n",
    "\n",
    "plot_hemispheres(surf_lh, surf_rh, PcorrAmyg, color_bar=True, color_range=(0, 0.15),\n",
    "        label_text=['amygdala'], cmap=\"viridis\", \n",
    "        embed_nb=True, size=(1400, 400), zoom=1.3, nan_color=(0.7, 0.7, 0.7, 1), \n",
    "        cb__labelTextProperty={\"fontSize\": 12}, interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e588b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes hemisphere plots of z-scored average pearson correlation of all subject timeseries FOR U1 AND U2 TOP AND BOTTOM 25%\n",
    "from brainspace.plotting import plot_hemispheres\n",
    "from brainspace.utils.parcellation import map_to_labels\n",
    "from brainspace.datasets import load_group_fc, load_parcellation, load_conte69\n",
    "import brainstat.mesh.data as mesh\n",
    "from brainstat.datasets import fetch_mask, fetch_template_surface\n",
    "from brainstat.tutorial.utils import fetch_mics_data\n",
    "corr_all_subs=np.zeros((2,10,64984))\n",
    "for count, i in enumerate(subjList):\n",
    "    if i<10:\n",
    "        subject = str('PNC00'+str(i))\n",
    "    else:\n",
    "        subject = str('PNC0'+str(i))\n",
    "    path=str(outDir1+subject+'_U1U2_fisherZ_top_bot_'+str(thrshhld)+'_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    Pcorr=np.loadtxt(path)\n",
    "    Pcorr=Pcorr[:2][:]\n",
    "\n",
    "    corr_all_subs[0][count][:]=Pcorr[0][:]\n",
    "    corr_all_subs[1][count][:]=Pcorr[1][:]\n",
    "    #cor=map_to_labels(Pcorr)\n",
    "\n",
    "Pcorr[0,:]=np.average(corr_all_subs[0][:][:],axis=0)\n",
    "Pcorr[1,:]=np.average(corr_all_subs[1][:][:],axis=0)\n",
    "\n",
    "surf_lh, surf_rh = load_conte69()\n",
    "\n",
    "#get the Yeo parcellations in order to identify the medial wall and turn the values inside into None for plotting purposes\n",
    "from nibabel import gifti\n",
    "ParcelDir='/data/mica1/03_projects/jessica/micasoft/parcellations/fs_LR-conte69/maps/'\n",
    "YeoLHPath=str(ParcelDir+'lh.Yeo2011_7Networks_N1000.label.gii')\n",
    "YeoRHPath=str(ParcelDir+'rh.Yeo2011_7Networks_N1000.label.gii')\n",
    "YeoLH=nib.load(YeoLHPath)\n",
    "YeoRH=nib.load(YeoRHPath)\n",
    "tmp = YeoLH.agg_data()\n",
    "YeoLRH=np.append(tmp,YeoRH.agg_data())\n",
    "medial_wall=np.where(YeoLRH==0)\n",
    "print(medial_wall[0])\n",
    "for i in range(len(medial_wall[0])):\n",
    "    Pcorr[1][medial_wall[0][i]]=None\n",
    "    Pcorr[0][medial_wall[0][i]]=None\n",
    "\n",
    "    \n",
    "#uncomment this code if want to only show correclation values above 0.2 and below -0.2\n",
    "#for i in range(len(Pcorr[0][:])):\n",
    "#    if Pcorr[0][i]>-0.1 and Pcorr[0][i]<0.1:\n",
    "#        Pcorr[0][i]=None\n",
    "#    if Pcorr[1][i]>-0.1 and Pcorr[1][i]<0.1:\n",
    "#        Pcorr[1][i]=None\n",
    "\n",
    "    #plot the hemispheres with correlation values betwen both regions of the amygdala and the cortex\n",
    "plot_hemispheres(surf_lh, surf_rh, Pcorr[:2,:], color_bar=True, color_range=(-0.15, 0.15),\n",
    "        label_text=['U1top25','U1bot25'], cmap=\"bwr\", \n",
    "        embed_nb=True, size=(1200, 700), zoom=1.2, nan_color=(0.7, 0.7, 0.7, 1), \n",
    "        cb__labelTextProperty={\"fontSize\": 12}, interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual test to see the different probability maps of each subject in plot hemispheres for U1 and U2\n",
    "snap_20=np.zeros((20,64984))\n",
    "for i in range(10):\n",
    "    snap_20[i*2]=corr_all_subs[0][i][:]\n",
    "    snap_20[i*2+1]=corr_all_subs[1][i][:]\n",
    "\n",
    "print(snap_20.shape)\n",
    "print(snap_20[0])\n",
    "#only showing 17 of 20 different datasets since plot_hemispheres does not work with more\n",
    "plot_hemispheres(surf_lh, surf_rh, snap_20[:17,:], color_bar=True, color_range=(0, 0.25),\n",
    "        label_text=['1U1top25','1U1bot25','4U1top25','4U1bot25','5U1top25','5U1bot25','6U1top25','6U1bot25','7U1top25','7U1bot25','9U1top25','9U1bot25','10U1top25','10U1bot25','11U1top25','11U1bot25','12U1top25'],#'U1bot25','U1top25','U1bot25'],\n",
    "        cmap=\"viridis\", embed_nb=True, size=(1000, 2800), zoom=1.2,cb__labelTextProperty={\"fontSize\": 12},interactive=False,nan_color=(0.7, 0.7, 0.7, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05713b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "###make mixed effects model for the timeseries treat subjects as random variable and look at differences between segments (Only analysis on U1)\n",
    "thrshhld=25\n",
    "gauss=10\n",
    "mmMatrix=np.zeros((40,64984))\n",
    "counter=0\n",
    "for i in subjList:\n",
    "    if i<10:\n",
    "        subject = str('PNC00'+str(i))\n",
    "    else:\n",
    "        subject = str('PNC0'+str(i))\n",
    "    path=str(outDir1+subject+'_U1U2_fisherZ_top_bot_'+str(thrshhld)+'_with_cortex_gs'+str(gauss)+'.txt')\n",
    "    #path=str(outDir1+subject+'_U1U2_fisherZ_top_bot_'+str(thrshhld)+'_with_cortex_test.txt')\n",
    "    fisherZ=np.loadtxt(path)\n",
    "    mmMatrix[counter,:]=fisherZ[0,:]\n",
    "    mmMatrix[counter+9,:]=fisherZ[2,:]\n",
    "    mmMatrix[counter+18,:]=fisherZ[1,:]\n",
    "    mmMatrix[counter+27,:]=fisherZ[3,:]\n",
    "    counter=counter+1\n",
    "#isolate only U1 data since further analysis will only be with U1\n",
    "mmMatrixU1=np.delete(mmMatrix,[10,11,12,13,14,15,16,17,18,19],axis=0)\n",
    "mmMatrixU1=np.delete(mmMatrixU1,[-10,-9,-8,-7,-6,-5,-4,-3,-2,-1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make mixed effects model of different network connectivities between subregions while accounting for different subjects\n",
    "from brainstat.stats.terms import MixedEffect, FixedEffect\n",
    "from brainstat.stats.SLM import SLM\n",
    "import pandas as pd\n",
    "from brainstat.datasets import fetch_mask, fetch_template_surface\n",
    "mask=fetch_mask('fslr32k')\n",
    "surf_combined = fetch_template_surface('fslr32k', join=True)\n",
    "\n",
    "subU1=np.array(['sub1','sub1','sub2','sub2','sub3','sub3','sub4','sub4','sub5','sub5','sub6','sub6','sub7','sub7','sub8','sub8','sub9','sub9','sub10','sub10'])\n",
    "segU1=np.array(['upper','lower','upper','lower','upper','lower','upper','lower','upper','lower','upper','lower','upper','lower','upper','lower','upper','lower','upper','lower'])\n",
    "\n",
    "attributes={'sub':subU1,'seg':segU1}\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data=mmMatrixU1)\n",
    "term_sub = MixedEffect(attributes['sub'])\n",
    "\n",
    "term_seg = FixedEffect(attributes['seg'])\n",
    "contrast_seg=(segU1 == \"upper\").astype(int) - (segU1 == \"lower\").astype(int)\n",
    "model_mixed = term_sub + term_seg\n",
    "\n",
    "slm_mixed = SLM(\n",
    "    model_mixed,\n",
    "    contrast_seg,\n",
    "    surf='fslr32k',\n",
    "    mask=mask,\n",
    "    correction=[\"rft\"],\n",
    "    cluster_threshold=0.01,\n",
    "    two_tailed=False,\n",
    ")\n",
    "\n",
    "slm_mixed.fit(mmMatrixU1)\n",
    "#skwn, krts = slm_mixed.qc(mmMatrixU1, v=87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the t-values of differences in connectivity for all regions of cortex \n",
    "plot_hemispheres(surf_lh, surf_rh, slm_mixed.t, color_bar=True, color_range=(-4,4),\n",
    "        label_text=[\"t-values\"], cmap=\"bwr\", embed_nb=True, size=(1200, 350), zoom=1.2,\n",
    "        nan_color=(0.7, 0.7, 0.7, 1), cb__labelTextProperty={\"fontSize\": 12}, interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slm_mixed.P[\"pval\"][\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28083182",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = [np.copy(slm_mixed.P[\"pval\"][\"C\"])]\n",
    "[np.place(x, np.logical_or(x > 0.05, ~mask), np.nan) for x in cp]\n",
    "sigT=[np.copy(slm_mixed.P[\"pval\"][\"C\"])]\n",
    "for i,val in enumerate(slm_mixed.P[\"pval\"][\"C\"]):\n",
    "    if val < 0.05:\n",
    "        sigT[0][i]=slm_mixed.t[0][i]\n",
    "    else:\n",
    "        sigT[0][i]=np.nan\n",
    "\n",
    "plot_hemispheres(surf_lh, surf_rh, sigT, color_bar=True, color_range=(3,5),\n",
    "        label_text=[\"Cluster p-values\"]#, \"Peak p-values\", \"Vertex p-values\"]\n",
    "        , cmap=\"hot\", embed_nb=True, size=(1200, 350), zoom=1.2, nan_color=(0.7, 0.7, 0.7, 1), \n",
    "        cb__labelTextProperty={\"fontSize\": 12}, interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8587215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which regions are significantly different\n",
    "cp = [np.copy(slm_mixed.P[\"pval\"][\"C\"])]\n",
    "[np.place(x, np.logical_or(x > 0.05, ~mask), np.nan) for x in cp]\n",
    "\n",
    "pp = [np.copy(slm_mixed.P[\"pval\"][\"P\"])]\n",
    "[np.place(x, np.logical_or(x > 0.05, ~mask), np.nan) for x in pp]\n",
    "\n",
    "vals = np.vstack([cp[0].T, pp[0].T])\n",
    "\n",
    "#qp = [np.copy(slm_mixed.Q)]\n",
    "#[np.place(x, np.logical_or(x > 0.05, ~mask), np.nan) for x in qp]\n",
    "\n",
    "#vals = np.vstack([cp[0].T, pp[0].T, qp[0].T])\n",
    "\n",
    "plot_hemispheres(surf_lh, surf_rh, vals[0], color_bar=True, color_range=(0, 0.1),\n",
    "        label_text=[\"Cluster p-values\"]#, \"Peak p-values\", \"Vertex p-values\"]\n",
    "        , cmap=\"binary\", embed_nb=True, size=(1200, 350), zoom=1.2, nan_color=(0,0,0,1), \n",
    "        cb__labelTextProperty={\"fontSize\": 12}, interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slm_mixed.P[\"clus\"])\n",
    "print(slm_mixed.P[\"peak\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the hemisphere with significant peak and cluster pvalue areas\n",
    "cp = [np.copy(slm_mixed.P[\"pval\"][\"C\"])]\n",
    "[np.place(x, np.logical_or(x > 0.05, ~mask), np.nan) for x in cp]\n",
    "\n",
    "pp = [np.copy(slm_mixed.P[\"pval\"][\"P\"])]\n",
    "[np.place(x, np.logical_or(x > 0.05, ~mask), np.nan) for x in pp]\n",
    "\n",
    "#qp = [np.copy(slm_mixed.Q)]\n",
    "#[np.place(x, np.logical_or(x > 0.05, ~mask), np.nan) for x in qp]\n",
    "\n",
    "vals = np.vstack([cp[0].T, pp[0].T])#, qp[0].T])\n",
    "\n",
    "plot_hemispheres(surf_lh, surf_rh, vals, color_bar=True, color_range=(0, 0.05),\n",
    "        label_text=[\"Cluster p-values\", \"Peak p-values\"], cmap=\"autumn_r\", \n",
    "        embed_nb=True, size=(1400, 400), zoom=1.8, nan_color=(0.7, 0.7, 0.7, 1), \n",
    "        cb__labelTextProperty={\"fontSize\": 12}, interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23da91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
